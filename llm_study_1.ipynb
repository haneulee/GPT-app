{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Results saved to research_result.txt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from duckduckgo_search import DDGS\n",
    "import wikipedia\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4-1106-preview\")\n",
    "\n",
    "def search_wikipedia(query):\n",
    "    try:\n",
    "        return wikipedia.summary(query, sentences=3)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return f\"Disambiguation error: {e.options}\"\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        return \"No page found.\"\n",
    "\n",
    "def search_duckduckgo(query):\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            results = [result['href'] for result in ddgs.text(query, max_results=3)]\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return [f\"Error fetching search results: {str(e)}\"]\n",
    "\n",
    "def scrape_website(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
    "        return text[:2000]\n",
    "    except Exception as e:\n",
    "        return f\"Error scraping {url}: {str(e)}\"\n",
    "\n",
    "def save_to_file(content, filename=\"research_result.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "    return f\"Results saved to {filename}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(name=\"Wikipedia Search\", func=search_wikipedia, description=\"Search Wikipedia for summaries.\"),\n",
    "    Tool(name=\"DuckDuckGo Search\", func=search_duckduckgo, description=\"Search DuckDuckGo for web results.\"),\n",
    "    Tool(name=\"Web Scraper\", func=scrape_website, description=\"Scrape a website's text content.\"),\n",
    "    Tool(name=\"File Saver\", func=save_to_file, description=\"Save research results to a text file.\")\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools=tools, llm=llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)\n",
    "\n",
    "query = \"Research about the XZ backdoor\"\n",
    "wikipedia_result = search_wikipedia(query)\n",
    "duckduckgo_results = search_duckduckgo(query)\n",
    "\n",
    "scraped_content = \"\"\n",
    "for url in duckduckgo_results:\n",
    "    scraped_content += scrape_website(url) + \"\\n\\n\"\n",
    "\n",
    "research_result = f\"Wikipedia Result:\\n{wikipedia_result}\\n\\nScraped Web Content:\\n{scraped_content}\"\n",
    "\n",
    "save_to_file(research_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
